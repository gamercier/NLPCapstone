---
title: "misc"
author: "Gustavo Mercier"
date: "August 25, 2016"
output: html_document
---
The default character encoding in my computer is en_US.UTF-8. This encoding includes the traditional ASCII characters as a subset. For details of the ASCII character set see
<http://www.ascii-code.com/>. For details of UTF-8 see <https://www.wikiwand.com/en/UTF-8>



The ASCII character set is single byte encoding. The unix/linux utility `{wc}` can count characters with single byte encoding as is the case with ASCII text using `wc -c`, but the count can be extended to include multi-byte characters using a different option, `wc -m`. If only single character encoding is present, then the output of both options should be the same. This can be used to test for multi-byte characters not present in the ASCII code.

We can get rid of lines that contain non-ascii characters by using regular expressions and utilities like `awk`. Regular expressions encapsulate groups of characters by using character classes (<http://www.regular-expressions.info/posixbrackets.html>)

Let's look at an example. The text below has three lines using three different forms of apostrophes. (For look up of unicode code for non-ascii characters see <http://unicodelookup.com/>)

```
This is case one: apostrophe's. This one is ascii code 39 (in decimal).
This is case two: apostrophe"s. This one is ascii code 34 (in decimal).
This is case three: apostrophe“s. This one is non-ascii Unicode 0x201D.
```

This test is saved in file test2.txt.

The following command line in linux or OS X will print only the first two lines.

```
LC_CTYPE=C awk '! /[^[:alnum:][:space:][:punct:]]/' test2.txt
```

## Raw File Features

Using linux/unix/OS X system commands like `wc`, `file`, `ls`, and `awk` we can get a quick feeling for the raw corpus files as shown in the table below. These commands work faster than built-in R equivalents.

```{r echo=FALSE}
library(stringr)
library(formattable)

prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
download.dir <- "nlpData.dir"; sub.dir <- "final"; corpus.dir <- "en_US"
corpus.dir <- file.path(prj.dir,download.dir,sub.dir,corpus.dir)
setwd(corpus.dir)

text.files <- dir(corpus.dir) #corpus.dir set in toCorpusDir.R

# system specific commands
arg <- c("-lh",text.files) ; size <- system2("ls",arg,stdout=TRUE)
lwb <- system2("wc",text.files,stdout=TRUE)
arg <- c("-m",text.files) ;  multi <- system2("wc",arg,stdout=TRUE)
arg <- c(text.files) ;  enc <- system2("file",arg,stdout=TRUE)

size <- sapply(str_split(size,boundary("word")),function(x) x[7])
c.single <- sapply(str_split(lwb[1:3],boundary("word")),function(x) x[3])
c.multi <- sapply(str_split(multi[1:3],boundary("word")),function(x) x[1])
words <- sapply(str_split(lwb[1:3],boundary("word")),function(x) x[2])
lines <- sapply(str_split(lwb[1:3],boundary("word")),function(x) x[1])
encoding <- sapply(str_split(enc,":"),function(x) str_trim(x[2]))

finfo <-data.frame("File"=text.files, "Size"=size, "Characters.Single"=c.single, "Characters.Multi"=c.multi, "Words"=words, "Lines"=lines, "Encoding"=encoding)

# sample text
sample.txt <- system2("head","-n 3 en_US.blogs.txt",stdout=TRUE)

# maximum number of characters in a twitter file line and the corresponding line
arg <- "'BEGIN {mx=0; mxtxt=\"\"}; {n=length($0); if(mx<n){mx=n; mxtxt=$0}}; END {print mx; print mxtxt}' en_US.twitter.txt"
maxTwitTxt <- system2("awk",arg,stdout=TRUE)

format_table(finfo)
```
***
The blogs and news are similar in size, ca. 200 Mb. The twitter file is smaller at 159 Mb. The count for single byte characters is not the same as for multi-byte characters. This suggests that the encoding is not simple ASCII code. The command `file` leads to the output in the last column where the encoding is shown as UTF-8. Moreover, the presence of long lines suggests multiple sentences per line of input. Also, the line terminations are based on Windows carriage return and line feeds ("\\r\\n") as opposed to Unix termination of line feeds ("\\n").

A quick view of the en_US.blogs.txt confirms these suspicions:

```{r echo=FALSE}
print(sample.txt)
```

First, the first line shows an example of non-ASCII character, the quote character. The following text illustrates the point:

```
This is case one: 'quote'. This one is ascii code 39 (in decimal).
This is case two: "quote". This one is ascii code 34 (in decimal).
This is case three: “quote“. This one is non-ascii Unicode 0x201D (hexadecimal).
```

Second, the carriage return ("\\r") is displayed as an additional character since I work under a OS X operating system. Finally, there are multiple sentences in input line 3.

It is also interesting that the maximum number of characters in a twitter line is `r maxTwitTxt[1]`. Because this exceeds 140 the twitter file also includes multiple twits per line! The line with the maximum number of characters also shows non-ascii japanese characters!

```{r echo=FALSE}
print(maxTwitTxt[2])
```

I can use the unix/linux/OS X utility `iconv` to convert the files from UTF-8 to US-ASCII. The utility `dos2unix` can be used to convert the line termination from "\\r\\n" to just "\\n". I rename the files by replacing the prefix "en_US" with the prefix "unix". These new files have the following features:

```{r echo=FALSE}
prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
download.dir <- "nlpData.dir"; sub.dir <- "final"; corpus.dir <- "en_US"
corpus.dir <- file.path(prj.dir,download.dir,sub.dir,corpus.dir)
setwd(corpus.dir)

# converting files from UTF-8 to US-ASCII and from dos termination to unix
c1 <- "-s --from-code UTF-8 --to-code US-ASCII -c"
ascii.files <- c()
for(f in text.files){
  ts <- unlist(str_split(f,"[.]"))
  new.f <- paste("ascii",ts[2],ts[3],sep=".")
  ascii.files <- c(ascii.files, new.f)
  arg <- paste(c1,f," > ",new.f)
  if(!file.exists(new.f)) system2("iconv",arg)
}

unix.files <- c()
for(f in ascii.files){
  ts <- unlist(str_split(f,"[.]"))
  new.f <- paste("unix",ts[2],ts[3],sep=".")
  unix.files <- c(unix.files, new.f)
  arg <- paste("-f -n ",f,new.f)
  if(!file.exists(new.f)) system2("dos2unix",arg)
}

text.files <- unix.files

# system specific commands
arg <- c("-lh",text.files) ; size <- system2("ls",arg,stdout=TRUE)
lwb <- system2("wc",text.files,stdout=TRUE)
arg <- c("-m",text.files) ;  multi <- system2("wc",arg,stdout=TRUE)
arg <- c(text.files) ;  enc <- system2("file",arg,stdout=TRUE)

size <- sapply(str_split(size,boundary("word")),function(x) x[7])
c.single <- sapply(str_split(lwb[1:3],boundary("word")),function(x) x[3])
c.multi <- sapply(str_split(multi[1:3],boundary("word")),function(x) x[1])
words <- sapply(str_split(lwb[1:3],boundary("word")),function(x) x[2])
lines <- sapply(str_split(lwb[1:3],boundary("word")),function(x) x[1])
encoding <- sapply(str_split(enc,":"),function(x) str_trim(x[2]))

finfo <-data.frame("File"=text.files, "Size"=size, "Characters.Single"=c.single, "Characters.Multi"=c.multi, "Words"=words, "Lines"=lines, "Encoding"=encoding)

# peeking at the file
sample.txt <- system2("head","-n 3 unix.blogs.txt",stdout=TRUE)

# maximum number of characters in a twitter file line and the corresponding line
arg <- "'BEGIN {mx=0; mxtxt=\"\"}; {n=length($0); if(mx<n){mx=n; mxtxt=$0}}; END {print mx; print mxtxt}' unix.twitter.txt"
maxTwitTxt <- system2("awk",arg,stdout=TRUE)

format_table(finfo)
```
****

Notice that now the number single byte characters and multi-byte characters is the same. The files coding is US-ASCII English. The file terminations are fixed, too.


Looking at the text in the blogs file we have

```{r echo=FALSE}
print(sample.txt)
```

Finding the largest line in the twitter file we get:

```{r echo=FALSE}
print(maxTwitTxt[2])
```

This line has `r maxTwitTxt[1]` characters!

***
