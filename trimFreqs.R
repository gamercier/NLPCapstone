# Capstone Project
# File: trimFreqs.R
#   Trims the Frequency vector by dropping elements thate occur less than a user
#   defined threshold.
#   Scheme 1 - only one implemented
#      Selects a given cut off for the cumulative frequency of unigram.
#      Deletes unigrams that fall out of the 0-cum.freq.cut.off
#      Gets the maximum frequency of unigrams cut out and uses it as a hard
#      cut off to delete higher order unigrams that occur with lower frequency.

print("Started script: trimFreqs.R")
# Got to project directory and load tools
prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
setwd(prj.dir)
print(paste("Current directory: ",getwd()))

# load resources
source("nlpTools.R")
print("Loaded tools.")

#Switch to processed corpus directory
source("toProcCorpusDir.R")
print(paste("Switched to diretory",getwd()))

#### Select freqs directory.          -- THIS IS REQUIRED
freqs.dir <- c("../75.dir")

#### Select frequency file to load.   -- THIS IS REQUIRED
freqs.file <- "freqs.dense.r"

#### Set frequecies to trim.          --THIS IS REQUIRED
db.element <- "seventyfive.pct"

#### SELECT percentage of unigrams to keep  -- THIS IS REQUIRED
pct.to.keep <- 0.80

##### REQUIRED: SELECT SCHEME TO USE #### ONLY ONE ON AT A TIME #####
scheme1 <- TRUE

#### SELECT OUTPUT FILE NAME                -- THIS IS REQUIRED
save.file <- "freqs.trimmed.dense.r"

#
print(paste("Going to directory containing frequecy data: ",freqs.dir))
setwd(freqs.dir)
print(paste("Switched to directory",getwd()))

print("Loading frequency data")
load(freqs.file)
print(paste("Loaded frequency data from",freqs.file))

### ALL THIS PRESUMES THE FREQUENCY DATA IS SORTED, MOST FREQUENT FIRST!!!!!! ####
### IF USED THE makeFreqs.R script will do this automatically ####
print("****** WARNING: PRESUMES FREQUENCY DATA IS SORTED, MOST FREQUENT FIRST *****")

freqs <- freqs.dense.db[[db.element]]   # freqs for one sample.

print("Computing Fractional Cummulative Frequency")
frac.cum <- lapply(freqs, function(x) cumsum(x)/sum(x))

############################ scheme one:
#   keep words that make up 99% of the total number of unigrams = dict.trimmed
#   Find the maximum count (frequency) seen in the words that are dropped.
#   Drop all bigrams, trigrams, and quadgrams with frequencies that are less than
#   that count.

findIndex <- function(freq.unigram, pct=0.99){
  # at and below this index you have 0.9 of the total number of unigrams
  fracCum.unigram <- cumsum(freq.unigram)/sum(freq.unigram)
  return(sum((fracCum.unigram <= pct)))
}
divideWords <- function(freq.unigram, pct=0.99){
  idx <- findIndex(freq.unigram,pct)
  words <- names(freq.unigram)
  return(list(words.to.keep=words[1:idx],
              words.to.cut =words[(idx+1):length(words)],index=idx))
}
trimFreqV <- function(freqV,cut.level=7){  # acts on freq vector for a unigram
  return(freqV[freqV > cut.level])
}

if(scheme1 == TRUE){
  print("Trimming using scheme one")
  print(paste("Trimming keeping",pct.to.keep*100,"% of the top unigrams."))
  word.partition <- divideWords(freqs$unigram,pct.to.keep)
  print(paste("Index for word (unigram) covering",pct.to.keep*100,"% is",
              word.partition$index))
  print(paste("Confirmation from fractional cummulative frequency:",
              round(frac.cum$unigram[word.partition$index],3)*100,"%"))
  cut.index <- word.partition$index
  bad.words <- word.partition$words.to.cut
  bad.word.max.count <- max(freqs$unigram[bad.words])
  
  print(paste("Maximum frequency of words to cut is",bad.word.max.count ,"."))
  print("Dropping all ngrams with frequency equal or less than this value.")
  # notice the maximum frequency in the bad words (words to cut)
  # can be the same as the minimum frequency of word to keep.
  # This is because numerous words may have the same frequency, and we
  # pick the cut off point to reach certain cummulative frequency. Which words
  # are kept depends on the way the sort function works in R.
  
  freqs.trimmed <- list()
  freqs.trimmed[[1]] <- freqs[[1]][1:(word.partition$index)]
  for(n in seq(2,length(freqs))){ # now do bigrams and up
    freqs.trimmed[[n]] <- freqs[[n]][freqs[[n]] > bad.word.max.count ]
  }
  names(freqs.trimmed) <- names(freqs)
  
  # build a database
  ngrams.trimmed <- lapply(freqs.trimmed,names)

  # checking that all words in the bi,tri, and quadgrams are in the vocabulary
  # generated by the unigrams.
  
  quad.words <- unique(unlist(toWords(ngrams.trimmed$quadgram)))
  tri.words <- unique(unlist(toWords(ngrams.trimmed$trigram)))
  bi.words <- unique(unlist(toWords(ngrams.trimmed$bigram)))
  
  no.bad.quads.words <- sum(!(quad.words %in% ngrams.trimmed$unigram))
  no.bad.tris.words <- sum(!(tri.words %in% ngrams.trimmed$unigram))
  no.bad.bis.words <- sum(!(bi.words %in% ngrams.trimmed$unigram))
  
  print(paste("The number of missing words in ngram=",c(2,3,4),"is",
              c(no.bad.bis.words,no.bad.tris.words,no.bad.quads.words)))
  
  ### must check this change in definition of dbs - made 2016-09-17
  freqs.trimmed.dense.db    <- list()
  freqs.trimmed.dense.db[[db.element]]    <- freqs.trimmed

  print("Saving frequencies, etc...")
  if(file.exists(save.file)){
    file.remove(save.file)
  }
  
  print(paste("Saving trimmed frequency database in file ",save.file))
  
  save(freqs.trimmed.dense.db, file=save.file)
  
  print("Finished trimming and saving database with scheme one.")
}

print("Returning to top directory")
setwd(prj.dir)
print(getwd())

