}
n <- n-1
}
guesses <- top3(scores)
data.frame(guess=unlist(getLastWord(names(guesses)))[1:3],
scores=guesses[1:3],row.names=c("1st","2nd","3rd"),
stringsAsFactors = FALSE)
}
guess.sb <- simple_guess.sb
##########  automating testing
testing <- function(test.s){
hit <- function(x){
output <- guess.sb(unlist(dropLastWord(x)))
if(unlist(getLastWord(x)) %in% output$guess){
return(list(hit=TRUE,answer=output))
} else {
return(list(hit=FALSE,answer=output))
}
}
test.f <- lapply(test.s,hit)
names(test.f) <- as.character(1:length(test.s))
return(test.f)
}
# Load the testing data
#  test_quads_dev.r -- test data for development, 1000 quads news text, non web
#     source: 12dicts-6.0.2_bench_1.zip
#     http://www.anc.org/OANC/OANC_GrAF.zip
#     This is from the American National Corpus: http://www.anc.org/
#  test_web_dev.r -- test data for development, 1000 quads from HCcorpora
#  test_web_test.r -- test data for final testing, 1000 quads from HCcorpora
source("toTesting.R")
##### Set test data set - REQUIRED
test.file <- "test_web_dev.r"
test.file <- file.path(getwd(),test.file)
load(test.file)
rm(list=ls())
source('~/git/NLPCapstone/testGuess.R')
rm(list=ls())
source('~/git/NLPCapstone/testGuess.R')
str(test.quads.dev)
tail(unlist(strsplit(test.file,"/")),1)
rm(list=ls())
source('~/git/NLPCapstone/testGuess.R')
rm(list=ls())
source('~/git/NLPCapstone/testGuess.R')
rm(list=ls())
source('~/git/NLPCapstone/testGuess.R')
rm(list=ls())
source('~/git/NLPCapstone/testGuess.R')
source('~/git/NLPCapstone/trimFreqs.R')
rm(list=ls())
source('~/git/NLPCapstone/makeScoresSB.R')
rm(list=ls())
source('~/git/NLPCapstone/testGuess.R')
rm(list=ls())
source('~/git/NLPCapstone/testGuess.R')
library(shiny); print(source('~/git/nextWordApp/nextWord.R')$value)
getwd()
print(source('~/git/nextWordApp/nextWord.R')$value)
library(shiny); print(source('~/git/nextWordApp/nextWord.R')$value)
shiny::runApp('~/git/nextWordApp')
runApp('~/git/nextWordApp')
getwd()
# Capstone Project
# File: trimDTMS.R
#   Trims the Document Text Matrix by reducing the sparsity
print("Started script: trimDTM.R")
# Got to project directory and load tools
prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
setwd(prj.dir)
print(paste("Current directory: ",getwd()))
# load resources
source("nlpTools.R")
print("Loaded tools.")
#Switch to processed corpus directory
source("toProcCorpusDir.R")
print(paste("Switched to diretory",getwd()))
#Select document text matrix directory. THIS IS REQUIRED
dtms.dir <- c("../100.dir")
print(paste("Going to directory containing document text matrix: ",dtms.dir))
setwd(dtms.dir)
print(paste("Switched to diretory",getwd()))
dir()
load("dtms_1.r")
str(dtms)
dtms.1 <- dtms
rm(dtms)
load("dtms_2.r")
str(dtms)
length(dtms)
length(dtms[[1]])
dtms[[1]][[1]] <- NULL
str(dtms)
names(dtms[[1]])
names(dtms[[1]]) <- "bigram"
str(dtms)
str(dtms.1)
save(dtms,file="dtms_2.r")
dtms.2 <- dtms
rm(dtms)
str(dtms.2)
load("dtms_3.r")
str(dtms)
dtms.3 <- dtms
rm(dtms)
load("dtms_4.r")
dtms.4 <- dtms
str(dtms.4)
rm(dtms)
dtms <- list("onehundred"=list(dtms.1[[1]],dtms.2[[1]],dtms.3[[1]],dtms.4[[1]]))
str(dtms)
dtms <- list("onehundred"=list(dtms.1[[1]]$unigram,dtms.2[[1]]$bigram,dtms.3[[1]]$trigram,dtms.4[[1]]$quadgram))
str(dtms)
names(dtms[[1]]) <- c("unigram","bigram","trigram","quadgram")
str(dtms)
save(dtms,file="dtms.r")
rm(dtms.1,dtms.2,dtms.3,dtms.4)
trimDTM <- function(dtm,sparsity=0.33){
dtm.dense <- list()
n <- length(dtm)
dtm.dense[1:n] <- lapply(dtm[1:n],removeSparseTerms,sparsity)
names(dtm.dense) <- names(dtm)
return(dtm.dense)
}
length(dtms[[1]])
dtms.dense <- lapply(dtms,trimDTM)
str(dtms.dense)
print("Saving dense dtms, etc...")
if(file.exists("dtmsDense.r")){
file.remove("dtmsDense.r")
}
save(dtms.dense,file="dtmsDense.r")
print("Finished saving dtms.dense stuff.")
print("Completed trimDTMS.R")
print("Resetting to project directory.")
setwd(prj.dir)
rm(list=ls())
# Capstone Project
# File: makeFreqs.R
#   Computes the frequencies from the document text matrices
#   Loads document text matrices from specific directories.
print("Started script: makeFreqs.R")
# Got to project directory and load tools
prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
setwd(prj.dir)
print(paste("Current directory: ",getwd()))
# load resources
source("nlpTools.R")
print("Loaded tools.")
#Switch to processed corpus directory
source("toProcCorpusDir.R")
print(paste("Switched to diretory",getwd()))
###### Select document text matrix directory. THIS IS REQUIRED
dtms.dir <- c("../100.dir")
###### Select name for sample. THIS IS REQUIRED
sample.name <- "onehundred"
###### SET SAVE FILE #####  THIS IS REQUIRED
save.file <- "freqs.dense.r"
print(paste("Going to directory containing Document Text Matrix: ",dtms.dir))
setwd(dtms.dir)
print(paste("Current directory: ",getwd()))
###### Select document text matrix to load. THIS IS REQUIRED
dtms.file <- c("dtmsDense.r")
# Loading document text matrices
load(dtms.file)
print("Loaded document text matrices")
##### Set value of dtms ### REQUIRED
dtms <- dtms.dense
# dtms is a list with one element per sample set. Each element has
# DTM for unigram ... quadgram (or higher depending on max.ngram)
print("Computing frequencies and sorting in decreasing order...")
freqs.db <- lapply(dtms,dtm2freq) # THIS RETURNS SORTED HIGHER FREQ FIRST
names(freqs.db) <- sample.name
str(freqs.db)
print("Saving frequencies, etc...")
if(file.exists(save.file)){
file.remove(save.file)
}
###### SET NAMES TO SAVE  ## REQUIRED
freqs.dense.db    <- freqs.db
save(freqs.dense.db,file=save.file)
print("Finished saving freqs.dense.db stuff.")
print("Completed makeFreqs.R")
print("Resetting to project directory.")
setwd(prj.dir)
dir()
rm(list=ls())
# Capstone Project
# File: trimFreqs.R
#   Trims the Frequency vector by dropping elements thate occur less than a user
#   defined threshold.
#   Scheme 1 - only one implemented
#      Selects a given cut off for the cumulative frequency of unigram.
#      Deletes unigrams that fall out of the 0-cum.freq.cut.off
#      Gets the maximum frequency of unigrams cut out and uses it as a hard
#      cut off to delete higher order unigrams that occur with lower frequency.
print("Started script: trimFreqs.R")
# Got to project directory and load tools
prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
setwd(prj.dir)
print(paste("Current directory: ",getwd()))
# load resources
source("nlpTools.R")
print("Loaded tools.")
#Switch to processed corpus directory
source("toProcCorpusDir.R")
print(paste("Switched to diretory",getwd()))
#### Select freqs directory.          -- THIS IS REQUIRED
freqs.dir <- c("../100.dir")
#### Select frequency file to load.   -- THIS IS REQUIRED
freqs.file <- "freqs.dense.r"
#### Set frequecies to trim.          --THIS IS REQUIRED
db.element <- "onehundred"
#### SELECT percentage of unigrams to keep  -- THIS IS REQUIRED
pct.to.keep <- 0.95
##### REQUIRED: SELECT SCHEME TO USE #### ONLY ONE ON AT A TIME #####
scheme1 <- TRUE
#### SELECT OUTPUT FILE NAME                -- THIS IS REQUIRED
save.file <- "freqs.trimmed.dense.r"
#
print(paste("Going to directory containing frequecy data: ",freqs.dir))
setwd(freqs.dir)
print(paste("Switched to directory",getwd()))
print("Loading frequency data")
load(freqs.file)
print(paste("Loaded frequency data from",freqs.file))
str(freqs.dense.r)
str(freqs.dense.db)
### ALL THIS PRESUMES THE FREQUENCY DATA IS SORTED, MOST FREQUENT FIRST!!!!!! ####
### IF USED THE makeFreqs.R script will do this automatically ####
print("****** WARNING: PRESUMES FREQUENCY DATA IS SORTED, MOST FREQUENT FIRST *****")
freqs <- freqs.dense.db[[db.element]]   # freqs for one sample.
print("Computing Fractional Cummulative Frequency")
frac.cum <- lapply(freqs, function(x) cumsum(x)/sum(x))
############################ scheme one:
#   keep words that make up 99% of the total number of unigrams = dict.trimmed
#   Find the maximum count (frequency) seen in the words that are dropped.
#   Drop all bigrams, trigrams, and quadgrams with frequencies that are less than
#   that count.
findIndex <- function(freq.unigram, pct=0.99){
# at and below this index you have 0.9 of the total number of unigrams
fracCum.unigram <- cumsum(freq.unigram)/sum(freq.unigram)
return(sum((fracCum.unigram <= pct)))
}
divideWords <- function(freq.unigram, pct=0.99){
idx <- findIndex(freq.unigram,pct)
words <- names(freq.unigram)
return(list(words.to.keep=words[1:idx],
words.to.cut =words[(idx+1):length(words)],index=idx))
}
trimFreqV <- function(freqV,cut.level=7){  # acts on freq vector for a unigram
return(freqV[freqV > cut.level])
}
if(scheme1 == TRUE){
print("Trimming using scheme one")
print(paste("Trimming keeping",pct.to.keep*100,"% of the top unigrams."))
word.partition <- divideWords(freqs$unigram,pct.to.keep)
print(paste("Index for word (unigram) covering",pct.to.keep*100,"% is",
word.partition$index))
print(paste("Confirmation from fractional cummulative frequency:",
round(frac.cum$unigram[word.partition$index],3)*100,"%"))
cut.index <- word.partition$index
bad.words <- word.partition$words.to.cut
bad.word.max.count <- max(freqs$unigram[bad.words])
print(paste("Maximum frequency of words to cut is",bad.word.max.count ,"."))
print("Dropping all ngrams with frequency equal or less than this value.")
# notice the maximum frequency in the bad words (words to cut)
# can be the same as the minimum frequency of word to keep.
# This is because numerous words may have the same frequency, and we
# pick the cut off point to reach certain cummulative frequency. Which words
# are kept depends on the way the sort function works in R.
freqs.trimmed <- list()
freqs.trimmed[[1]] <- freqs[[1]][1:(word.partition$index)]
for(n in seq(2,length(freqs))){ # now do bigrams and up
freqs.trimmed[[n]] <- freqs[[n]][freqs[[n]] > bad.word.max.count ]
}
names(freqs.trimmed) <- names(freqs)
# build a database
ngrams.trimmed <- lapply(freqs.trimmed,names)
# checking that all words in the bi,tri, and quadgrams are in the vocabulary
# generated by the unigrams.
quad.words <- unique(unlist(toWords(ngrams.trimmed$quadgram)))
tri.words <- unique(unlist(toWords(ngrams.trimmed$trigram)))
bi.words <- unique(unlist(toWords(ngrams.trimmed$bigram)))
no.bad.quads.words <- sum(!(quad.words %in% ngrams.trimmed$unigram))
no.bad.tris.words <- sum(!(tri.words %in% ngrams.trimmed$unigram))
no.bad.bis.words <- sum(!(bi.words %in% ngrams.trimmed$unigram))
print(paste("The number of missing words in ngram=",c(2,3,4),"is",
c(no.bad.bis.words,no.bad.tris.words,no.bad.quads.words)))
### must check this change in definition of dbs - made 2016-09-17
freqs.trimmed.dense.db    <- list()
freqs.trimmed.dense.db[[db.element]]    <- freqs.trimmed
print("Saving frequencies, etc...")
if(file.exists(save.file)){
file.remove(save.file)
}
print(paste("Saving trimmed frequency database in file ",save.file))
save(freqs.trimmed.dense.db, file=save.file)
print("Finished trimming and saving database with scheme one.")
}
print("Returning to top directory")
setwd(prj.dir)
print(getwd())
rm(list=ls())
# Capstone Project
# File: makeScoresSB.R
#   Computes the frequencies and scores for stupid backoff
#   Loads document frequency database for computations.
print("Started script: makeScoresSB.R")
# Got to project directory and load tools
prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
setwd(prj.dir)
print(paste("Current directory: ",getwd()))
# load resources
source("nlpTools.R")
print("Loaded tools.")
#Switch to processed corpus directory
source("toProcCorpusDir.R")
print(paste("Switched to directory",getwd()))
#Select freqs directory. THIS IS REQUIRED
freqs.dir <- c("../100.dir")
print(paste("Going to directory containing frequecy data: ",freqs.dir))
setwd(freqs.dir)
print(paste("Switched to directory",getwd()))
###### Select frequency file. THIS IS REQUIRED
freqs.file <- "freqs.trimmed.dense.r"
print(paste("Loading frequency data from file",freqs.file))
load(freqs.file)
print(paste("Loaded frequency data from file",freqs.file))
#### SELECT FILE FOR SAVING -- REQUIRED
save.file <- "scoresSB.trimmed.dense.r"
###### Set frequency database. THIS REQUIRED
freqs.db <- freqs.trimmed.dense.db
###### SEE BELOW TO Select save file for scores database -- THIS IS REQUIRED
# Define SCORING FUNCTION. Based on Stupid Back Off
#    score(ngram) = counts(ngram)/counts(ngram with first word dropped) if n >1
#    score(1gram) = counts(1gram)/(total number of 1grams (not unique 1grams))
#For stupid back up implementation:
#http://stackoverflow.com/questions/16383194/stupid-backoff-implementation-clarification
# NOTE: Computations here assume that for each sample,
#       the freqs.db is ordered as unigram, bigram, trigram, quadgram...
#
print("Computing ngram scores for stupid backoff scheme.")
scores.db <- lapply(freqs.db,score.sbackoff) # computes scores for each sample
print("Done computing scores.")
ngrams.db <- lapply(scores.db,function(x) lapply(x,names))
bases.db  <- lapply( ngrams.db,function(x) { lapply( x[2:length(x)],
function(y) unlist(dropLastWord(y)) ) } )
#### SET NAMES FOR SAVING - REQUIRED
scores.trimmed.dense.db <- scores.db
ngrams.trimmed.dense.db <- ngrams.db
bases.trimmed.dense.db  <- bases.db
if(file.exists(save.file)){
file.remove(save.file)
}
print(paste("Saving scores database in",save.file))
save(scores.trimmed.dense.db,
ngrams.trimmed.dense.db,
bases.trimmed.dense.db, file=save.file)
print(paste("Finished saving scores database in",save.file))
print("Completed makeScoresSB.R")
print("Resetting to project directory.")
setwd(prj.dir)
rm(list=ls())
# Capstone Project
# File: testGuess.R
# Tests guessing function against a database of quadgrams in nlpData.dir/testing
# Move to testing directory and load the test dataset
prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
print(paste("Moving to",prj.dir))
setwd(prj.dir)
print(paste("Current directory to",getwd()))
# helper functions
library(stringr)
# helper functions
toWords <- function(ngram) str_split(ngram,pattern=boundary("word"))
toStr <- function(words) lapply(words,str_c,collapse=" ")
dropFirstWord <- function(ngram) {
words <- toWords(ngram)
n <- length(words[[1]]) # no checks! Assumes all ngrams have same n
toStr(lapply(words,function(s) s[2:n]))
}
dropLastWord <- function(ngram){
words <- toWords(ngram)
toStr(lapply(words,function(s) { n<-length(s); s[1:(n-1)] }))
}
getLastWord <- function(ngram){
words <- toWords(ngram)
toStr(lapply(words,function(s) { tail(s,1) }))
}
countWords <- function(ngram) lapply(toWords(ngram),length)
toSpace <- function(x, pattern, ...){
return(gsub(pattern," ", x, ...))
}
toNone <- function(x, pattern, ...){
return(gsub(pattern,"", x, ...))
}
top3 <- function(v){
n <- length(v)
hits <- sort(v,partial=c(n,n-1,n-2))[n:(n-2)]
idx <- unlist(sapply(hits, function(x) which(v == x)))
return(v[idx])
}
# simple_guess_sb was modified 2016-09-18 stringsAsFactors = FALSE
simple_guess.sb <- function(base_ngram){
words <- unlist(toWords(base_ngram))
n <- length(words)   # size of base_ngram. ngram size is n+1
nMax <- n
scores <- (ALPHA^nMax)*TOP.UNI.SCORES # ngram is unigram (base_ngram is ""), n=0
while(n>0){
base <- paste(words[(nMax-n+1):nMax],collapse=" ")
hits <- base == basesDB[[n]]  # basesDB[[n]] has base for ngramsDB[[(n+1)]]
if(sum(hits)){
# scores <- c(scores,(ALPHA^(nMax-n))*scoresDB[[(n+1)]][ ngramsDB[[(n+1)]][hits] ])
scores <- c(scores,(ALPHA^(nMax-n))*scoresDB[[(n+1)]][ hits ])
}
n <- n-1
}
guesses <- top3(scores)
data.frame(guess=unlist(getLastWord(names(guesses)))[1:3],
scores=guesses[1:3], sources=names(guesses),
row.names=c("1st","2nd","3rd"),
stringsAsFactors = FALSE)
}
guess.sb <- simple_guess.sb
##########  automating testing
testing <- function(test.s){
hit <- function(x){
output <- guess.sb(unlist(dropLastWord(x)))
if(unlist(getLastWord(x)) %in% output$guess){
return(list(hit=TRUE,answer=output))
} else {
return(list(hit=FALSE,answer=output))
}
}
test.f <- lapply(test.s,hit)
names(test.f) <- as.character(1:length(test.s))
return(test.f)
}
# Load the testing data
#  test_quads_dev.r -- test data for development, 1000 quads news text, non web
#     source: 12dicts-6.0.2_bench_1.zip
#     http://www.anc.org/OANC/OANC_GrAF.zip
#     This is from the American National Corpus: http://www.anc.org/
#  test_web_dev.r -- test data for development, 1000 quads from HCcorpora
#  test_web_test.r -- test data for final testing, 1000 quads from HCcorpora
source("toTesting.R")
##### Set test data set - REQUIRED
test.file <- "test_web_dev.r"
dir()
##### Set test data set - REQUIRED
test.file <- "test_web_dev.r"
##### Set: scores database (wrk) directory,
#####      scores file
#####      save file
##### Set: scores database (wrk) directory,
#####      scores file
#####      save file
#####      database element - all REQUIRED ###########
wrk.dir <- "100.dir"
db.element <- "onehundred"
scores.file <- "scoresSB.trimmed.dense.r"
save.file <- "dev_test_web.r"
###
test.file <- file.path(getwd(),test.file)
load(test.file)
print(paste("Loading test data set file",test.file))
if(tail(unlist(strsplit(test.file,"/")),1) == "test_web_dev.r"){
test.sample <- test.quads.dev
}
print(paste("Loaded test data:",ls(pattern="sample")))
setwd(prj.dir)
print(paste("Returning directory to",getwd()))
##### Load the scores database and set the global variables
source("toDbs.R")
dir()
scores.dir <- file.path(dbs.dir,wrk.dir)
setwd(scores.dir) ; dir()
getwd()
load(scores.file)
print(paste("Loading scores file",scores.file))
print(paste("Loaded scores database:",
ls(pattern="^(bases|ngrams|scores)\\.[a-zA-Z0-9\\.]*db")))
scores.file
dir()
scores.dir <- file.path(dbs.dir,wrk.dir)
setwd(scores.dir) ; dir()
load(scores.file)
print(paste("Loading scores file",scores.file))
print(paste("Loaded scores database:",
ls(pattern="^(bases|ngrams|scores)\\.[a-zA-Z0-9\\.]*db")))
scoresDB <-  scores.trimmed.dense.db[[db.element]]
ngramsDB <-  ngrams.trimmed.dense.db[[db.element]]
basesDB  <-  bases.trimmed.dense.db[[db.element]]
TOP.UNI.SCORES <- scoresDB$unigram[1:3] # presumed to be sorted with higher score first
ALPHA <- 0.4
# Do the test
alphas <- c(0.2,0.4,0.6,0.8)
print(paste("Test:",tail(unlist(strsplit(test.file,"/")),1)
,"database:",db.element,"method: stupid backoff"))
print(paste("Scores database directory:",scores.dir))
print(paste("Scores file:",scores.file))
results <- list()
n.test <- length(test.sample)
for(alpha in alphas){
ALPHA <- alpha
results[[as.character(alpha)]] <- testing(test.sample)
n.hits <- sum(sapply(results[[as.character(alpha)]],function(x) x$hit))
print(paste("Hits: ",sum(n.hits)," out of ",n.test,"alpha=",alpha))
}
rm(list=ls())
source('~/git/NLPCapstone/trimFreqs.R')
