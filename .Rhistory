N.ngrams.trimmed <- lapply(freqs.trimmed,sum)
V.ngrams.trimmed <- lapply(ngrams.trimmed,length)
# checking that all words in the bi,tri, and quadgrams are in the vocabulary
# generated by the unigrams.
quad.words <- unique(unlist(toWords(ngrams.trimmed$quadgram)))
tri.words <- unique(unlist(toWords(ngrams.trimmed$trigram)))
bi.words <- unique(unlist(toWords(ngrams.trimmed$bigram)))
no.bad.quads.words <- sum(!(quad.words %in% ngrams.trimmed$unigram))
no.bad.tris.words <- sum(!(tri.words %in% ngrams.trimmed$unigram))
no.bad.bis.words <- sum(!(bi.words %in% ngrams.trimmed$unigram))
print(paste("The number of missing words in ngram=",c(2,3,4),"is",
c(no.bad.bis.words,no.bad.tris.words,no.bad.quads.words)))
freqs.trimmed.dense.db    <- vector(mode="list",1)
ngrams.trimmed.dense.db   <- vector(mode="list",1)
bases.trimmed.dense.db    <- vector(mode="list",1)
N.ngrams.trimmed.dense.db <- vector(mode="list",1)
V.ngrams.trimmed.dense.db <- vector(mode="list",1)
freqs.trimmed.dense.db[[db.element]]    <- freqs.trimmed
ngrams.trimmed.dense.db[[db.element]]   <- ngrams.trimmed
bases.trimmed.dense.db[[db.element]]    <- bases.trimmed
N.ngrams.trimmed.dense.db[[db.element]] <- N.ngrams.trimmed
V.ngrams.trimmed.dense.db[[db.element]] <- V.ngrams.trimmed
print("Saving frequencies, etc...")
if(file.exists(save.file)){
file.remove(save.file)
}
print(paste("Saving trimmed frequency database in file ",save.file))
save(freqs.trimmed.dense.db,
ngrams.trimmed.dense.db,
bases.trimmed.dense.db,
N.ngrams.trimmed.dense.db,
V.ngrams.trimmed.dense.db, file=save.file)
print("Finished trimming and saving database with scheme one.")
}
if(scheme2 == TRUE) print("Scheme 2 is not active. Sorry...")
print("Returning to top directory")
setwd(prj.dir)
print(getwd())
rm(list=ls())
# Capstone Project
# File: trimFreqs.R
#   Trims the Frequency vector by dropping elements thate occur less than a user
#   defined threshold.
#   Scheme 1 - only one implemented
#      Selects a given cut off for the cumulative frequency of unigram.
#      Deletes unigrams that fall out of the 0-cum.freq.cut.off
#      Gets the maximum frequency of unigrams cut out and uses it as a hard
#      cut off to delete higher order unigrams that occur with lower frequency.
print("Started script: trimFreqs.R")
# Got to project directory and load tools
prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
setwd(prj.dir)
print(paste("Current directory: ",getwd()))
# load resources
source("nlpTools.R")
print("Loaded tools.")
#Switch to processed corpus directory
source("toProcCorpusDir.R")
print(paste("Switched to diretory",getwd()))
#Select freqs directory. THIS IS REQUIRED
freqs.dir <- c("../75.dir")
print(paste("Going to directory containing frequecy data: ",freqs.dir))
setwd(freqs.dir)
print(paste("Switched to diretory",getwd()))
#Select frequency file to load. THIS IS REQUIRED
freqs.file <- "freqs.dense.r"
print("Loading frequency data")
load(freqs.file)
print(paste("Loaded frequency data from",freqs.file))
#### FIX database labels
names(freqs.dense.db) <- "seventyfive.pct"
names(ngrams.dense.db) <- "seventyfive.pct"
names(N.ngrams.dense.db) <- "seventyfive.pct"
names(V.ngrams.dense.db) <- "seventyfive.pct"
file.remove(freqs.file) # intention is to replace this one.
save(freqs.dense.db,
ngrams.dense.db,
bases.dense.db,
N.ngrams.dense.db,
V.ngrams.dense.db, file=freqs.file) # replacing.
#### FIX FINISHED
### ALL THIS PRESUMES THE FREQUENCY DATA IS SORTED, MOST FREQUENT FIRST!!!!!! ####
### IF USED THE makeFreqs.R script will do this automatically ####
print("****** WARNING: PRESUMES FREQUENCY DATA IS SORTED, MOST FREQUENT FIRST *****")
# set frequecies to trim. ########## THIS IS REQUIRED
db.element <- "seventyfive.pct"
freqs <- freqs.dense.db[[db.element]]   # freqs for one sample.
ngrams <- ngrams.dense.db[[db.element]] # ngrams for one sample, the ngrams with frequency in freqs
N.ngrams <- N.ngrams.dense.db[[db.element]]
V.ngrams <- V.ngrams.dense.db[[db.element]]
print("**** Statistics on the Frequency Data ****")
print("******************************************")
print("----- Totals -----")
total.ngrams <- as.vector(unlist(N.ngrams))
names(total.ngrams) <- names(N.ngrams)
voc.ngrams <- as.vector(unlist(V.ngrams.dense.db))
names(voc.ngrams) <- names(V.ngrams)
totals.df <- data.frame(n.voc=voc.ngrams,n.ngrams=total.ngrams)
print(totals.df)
for(ngram in names(freqs)){
print(paste("Minimum frequency in ",ngram," is ",min(freqs[[ngram]])))
}
# print("Computing and Plotting Frequency of Frequencies!")
# ff <- list(unigram=NULL,bigram=NULL,trigram=NULL,quadgram=NULL)
# for(ngram in names(freqs)){
#   ff[[ngram]] <- freq2ff(freqs[[ngram]])
# }
# #plotting histograms
# for(ngram in names(ff)){
#   plot(ff[[ngram]][1:200,],main=paste("Frequency of Frequencies - ",ngram))
# }
print("Computing Fractional Cummulative Frequency")
frac.cum <- lapply(freqs, function(x) cumsum(x)/sum(x))
# for(ngram in names(frac.cum)){
#   plot(x=1:1000,y=frac.cum[[ngram]][1:1000],
#        main=paste("Fractional Cummulative Frequency - ",ngram),
#        xlab="index",ylab="cummulative frequency")
# }
##### REQUIRED: SELECT SCHEME TO USE #### ONLY ONE ON AT A TIME #####
scheme1 <- TRUE
scheme2 <- FALSE
#### SELECT OUTPUT FILE NAME ### REQUIRED
save.file <- "freqs.trimmed.dense.r"
############################ scheme one:
#   keep words that make up 99% of the total number of unigrams = dict.trimmed
#   Find the maximum count (frequency) seen in the words that are dropped.
#   Drop all bigrams, trigrams, and quadgrams with frequencies that are less than
#   that count.
findIndex <- function(freq.unigram, pct=0.99){
# at and below this index you have 0.9 of the total number of unigrams
fracCum.unigram <- cumsum(freq.unigram)/sum(freq.unigram)
return(sum((fracCum.unigram <= pct)))
}
divideWords <- function(freq.unigram, pct=0.99){
idx <- findIndex(freq.unigram,pct)
words <- names(freq.unigram)
return(list(words.to.keep=words[1:idx],
words.to.cut =words[(idx+1):length(words)],index=idx))
}
trimFreqV <- function(freqV,cut.level=7){  # acts on freq vector for a unigram
return(freqV[freqV > cut.level])
}
if(scheme1 == TRUE){
print("Trimming using scheme one")
pct.to.keep <- 0.99 #### SELECT percentage of unigrams to keep
word.partition <- divideWords(freqs$unigram,pct.to.keep)
print(paste("Index for word (unigram) covering",pct.to.keep*100,"% is",
word.partition$index))
print(paste("Confirmation from fractional cummulative frequency:",
round(frac.cum$unigram[word.partition$index],3)*100,"%"))
cut.index <- word.partition$index
bad.words <- word.partition$words.to.cut
bad.word.max.count <- max(freqs$unigram[bad.words])
print(paste("Maximum frequency of words to cut is",bad.word.max.count ,"."))
print("Dropping all ngrams with frequency equal or less than this value.")
# notice the maximum frequency in the bad words (words to cut)
# can be the same as the minimum frequency of word to keep.
# This is because numerous words may have the same frequency, and we
# pick the cut off point to reach certain cummulative frequency. Which words
# are kept depends on the way the sort function works in R.
freqs.trimmed <- vector(mode="list",length=4)
names(freqs.trimmed) <- names(freqs)
freqs.trimmed$unigram <- freqs$unigram[1:word.partition$index]
for(n in seq(2,length(freqs.trimmed))){ # now do bigrams and up
freqs.trimmed[[n]] <- freqs[[n]][freqs[[n]] > bad.word.max.count ]
}
# build a database
ngrams.trimmed <- lapply(freqs.trimmed,names)
bases.trimmed <- vector(mode="list",length=3)
bases.trimmed <- lapply(ngrams.trimmed[2:length(ngrams.trimmed)],
function(x) unlist(dropLastWord(x)))
N.ngrams.trimmed <- lapply(freqs.trimmed,sum)
V.ngrams.trimmed <- lapply(ngrams.trimmed,length)
# checking that all words in the bi,tri, and quadgrams are in the vocabulary
# generated by the unigrams.
quad.words <- unique(unlist(toWords(ngrams.trimmed$quadgram)))
tri.words <- unique(unlist(toWords(ngrams.trimmed$trigram)))
bi.words <- unique(unlist(toWords(ngrams.trimmed$bigram)))
no.bad.quads.words <- sum(!(quad.words %in% ngrams.trimmed$unigram))
no.bad.tris.words <- sum(!(tri.words %in% ngrams.trimmed$unigram))
no.bad.bis.words <- sum(!(bi.words %in% ngrams.trimmed$unigram))
print(paste("The number of missing words in ngram=",c(2,3,4),"is",
c(no.bad.bis.words,no.bad.tris.words,no.bad.quads.words)))
freqs.trimmed.dense.db    <- vector(mode="list",1)
ngrams.trimmed.dense.db   <- vector(mode="list",1)
bases.trimmed.dense.db    <- vector(mode="list",1)
N.ngrams.trimmed.dense.db <- vector(mode="list",1)
V.ngrams.trimmed.dense.db <- vector(mode="list",1)
freqs.trimmed.dense.db[[db.element]]    <- freqs.trimmed
ngrams.trimmed.dense.db[[db.element]]   <- ngrams.trimmed
bases.trimmed.dense.db[[db.element]]    <- bases.trimmed
N.ngrams.trimmed.dense.db[[db.element]] <- N.ngrams.trimmed
V.ngrams.trimmed.dense.db[[db.element]] <- V.ngrams.trimmed
print("Saving frequencies, etc...")
if(file.exists(save.file)){
file.remove(save.file)
}
print(paste("Saving trimmed frequency database in file ",save.file))
save(freqs.trimmed.dense.db,
ngrams.trimmed.dense.db,
bases.trimmed.dense.db,
N.ngrams.trimmed.dense.db,
V.ngrams.trimmed.dense.db, file=save.file)
print("Finished trimming and saving database with scheme one.")
}
if(scheme2 == TRUE) print("Scheme 2 is not active. Sorry...")
print("Returning to top directory")
setwd(prj.dir)
print(getwd())
rm(list=ls())
# Capstone Project
# File: trimFreqs.R
#   Trims the Frequency vector by dropping elements thate occur less than a user
#   defined threshold.
#   Scheme 1 - only one implemented
#      Selects a given cut off for the cumulative frequency of unigram.
#      Deletes unigrams that fall out of the 0-cum.freq.cut.off
#      Gets the maximum frequency of unigrams cut out and uses it as a hard
#      cut off to delete higher order unigrams that occur with lower frequency.
print("Started script: trimFreqs.R")
# Got to project directory and load tools
prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
setwd(prj.dir)
print(paste("Current directory: ",getwd()))
# load resources
source("nlpTools.R")
print("Loaded tools.")
#Switch to processed corpus directory
source("toProcCorpusDir.R")
print(paste("Switched to diretory",getwd()))
#Select freqs directory. THIS IS REQUIRED
freqs.dir <- c("../50.dir")
print(paste("Going to directory containing frequecy data: ",freqs.dir))
setwd(freqs.dir)
print(paste("Switched to diretory",getwd()))
#Select frequency file to load. THIS IS REQUIRED
freqs.file <- "freqs.dense.r"
print("Loading frequency data")
load(freqs.file)
print(paste("Loaded frequency data from",freqs.file))
names(freqs.dense.db)
# set frequecies to trim. ########## THIS IS REQUIRED
db.element <- "fifty.pct"
freqs <- freqs.dense.db[[db.element]]   # freqs for one sample.
ngrams <- ngrams.dense.db[[db.element]] # ngrams for one sample, the ngrams with frequency in freqs
N.ngrams <- N.ngrams.dense.db[[db.element]]
V.ngrams <- V.ngrams.dense.db[[db.element]]
print("**** Statistics on the Frequency Data ****")
print("******************************************")
print("----- Totals -----")
total.ngrams <- as.vector(unlist(N.ngrams))
names(total.ngrams) <- names(N.ngrams)
voc.ngrams <- as.vector(unlist(V.ngrams.dense.db))
names(voc.ngrams) <- names(V.ngrams)
totals.df <- data.frame(n.voc=voc.ngrams,n.ngrams=total.ngrams)
print(totals.df)
for(ngram in names(freqs)){
print(paste("Minimum frequency in ",ngram," is ",min(freqs[[ngram]])))
}
print("Computing Fractional Cummulative Frequency")
frac.cum <- lapply(freqs, function(x) cumsum(x)/sum(x))
##### REQUIRED: SELECT SCHEME TO USE #### ONLY ONE ON AT A TIME #####
scheme1 <- TRUE
scheme2 <- FALSE
#### SELECT OUTPUT FILE NAME ### REQUIRED
save.file <- "freqs.trimmed.dense.r"
############################ scheme one:
#   keep words that make up 99% of the total number of unigrams = dict.trimmed
#   Find the maximum count (frequency) seen in the words that are dropped.
#   Drop all bigrams, trigrams, and quadgrams with frequencies that are less than
#   that count.
findIndex <- function(freq.unigram, pct=0.99){
# at and below this index you have 0.9 of the total number of unigrams
fracCum.unigram <- cumsum(freq.unigram)/sum(freq.unigram)
return(sum((fracCum.unigram <= pct)))
}
divideWords <- function(freq.unigram, pct=0.99){
idx <- findIndex(freq.unigram,pct)
words <- names(freq.unigram)
return(list(words.to.keep=words[1:idx],
words.to.cut =words[(idx+1):length(words)],index=idx))
}
trimFreqV <- function(freqV,cut.level=7){  # acts on freq vector for a unigram
return(freqV[freqV > cut.level])
}
if(scheme1 == TRUE){
print("Trimming using scheme one")
pct.to.keep <- 0.99 #### SELECT percentage of unigrams to keep
word.partition <- divideWords(freqs$unigram,pct.to.keep)
print(paste("Index for word (unigram) covering",pct.to.keep*100,"% is",
word.partition$index))
print(paste("Confirmation from fractional cummulative frequency:",
round(frac.cum$unigram[word.partition$index],3)*100,"%"))
cut.index <- word.partition$index
bad.words <- word.partition$words.to.cut
bad.word.max.count <- max(freqs$unigram[bad.words])
print(paste("Maximum frequency of words to cut is",bad.word.max.count ,"."))
print("Dropping all ngrams with frequency equal or less than this value.")
# notice the maximum frequency in the bad words (words to cut)
# can be the same as the minimum frequency of word to keep.
# This is because numerous words may have the same frequency, and we
# pick the cut off point to reach certain cummulative frequency. Which words
# are kept depends on the way the sort function works in R.
freqs.trimmed <- vector(mode="list",length=4)
names(freqs.trimmed) <- names(freqs)
freqs.trimmed$unigram <- freqs$unigram[1:word.partition$index]
for(n in seq(2,length(freqs.trimmed))){ # now do bigrams and up
freqs.trimmed[[n]] <- freqs[[n]][freqs[[n]] > bad.word.max.count ]
}
# build a database
ngrams.trimmed <- lapply(freqs.trimmed,names)
bases.trimmed <- vector(mode="list",length=3)
bases.trimmed <- lapply(ngrams.trimmed[2:length(ngrams.trimmed)],
function(x) unlist(dropLastWord(x)))
N.ngrams.trimmed <- lapply(freqs.trimmed,sum)
V.ngrams.trimmed <- lapply(ngrams.trimmed,length)
# checking that all words in the bi,tri, and quadgrams are in the vocabulary
# generated by the unigrams.
quad.words <- unique(unlist(toWords(ngrams.trimmed$quadgram)))
tri.words <- unique(unlist(toWords(ngrams.trimmed$trigram)))
bi.words <- unique(unlist(toWords(ngrams.trimmed$bigram)))
no.bad.quads.words <- sum(!(quad.words %in% ngrams.trimmed$unigram))
no.bad.tris.words <- sum(!(tri.words %in% ngrams.trimmed$unigram))
no.bad.bis.words <- sum(!(bi.words %in% ngrams.trimmed$unigram))
print(paste("The number of missing words in ngram=",c(2,3,4),"is",
c(no.bad.bis.words,no.bad.tris.words,no.bad.quads.words)))
freqs.trimmed.dense.db    <- vector(mode="list",1)
ngrams.trimmed.dense.db   <- vector(mode="list",1)
bases.trimmed.dense.db    <- vector(mode="list",1)
N.ngrams.trimmed.dense.db <- vector(mode="list",1)
V.ngrams.trimmed.dense.db <- vector(mode="list",1)
freqs.trimmed.dense.db[[db.element]]    <- freqs.trimmed
ngrams.trimmed.dense.db[[db.element]]   <- ngrams.trimmed
bases.trimmed.dense.db[[db.element]]    <- bases.trimmed
N.ngrams.trimmed.dense.db[[db.element]] <- N.ngrams.trimmed
V.ngrams.trimmed.dense.db[[db.element]] <- V.ngrams.trimmed
print("Saving frequencies, etc...")
if(file.exists(save.file)){
file.remove(save.file)
}
print(paste("Saving trimmed frequency database in file ",save.file))
save(freqs.trimmed.dense.db,
ngrams.trimmed.dense.db,
bases.trimmed.dense.db,
N.ngrams.trimmed.dense.db,
V.ngrams.trimmed.dense.db, file=save.file)
print("Finished trimming and saving database with scheme one.")
}
if(scheme2 == TRUE) print("Scheme 2 is not active. Sorry...")
print("Returning to top directory")
setwd(prj.dir)
print(getwd())
rm(list=ls())
# Capstone Project
# File: makeScoresSB.R
#   Computes the frequencies and scores for stupid backoff
#   Loads document frequency database for computations.
print("Started script: makeScoresSB.R")
# Got to project directory and load tools
prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
setwd(prj.dir)
print(paste("Current directory: ",getwd()))
# load resources
source("nlpTools.R")
print("Loaded tools.")
#Switch to processed corpus directory
source("toProcCorpusDir.R")
print(paste("Switched to directory",getwd()))
#Select freqs directory. THIS IS REQUIRED
freqs.dir <- c("../10.dir")
print(paste("Going to directory containing frequecy data: ",freqs.dir))
setwd(freqs.dir)
print(paste("Switched to directory",getwd()))
# Select frequency file. THIS IS REQUIRED
freqs.file <- "freqs.trimmed.dense.r"
print(paste("Loading frequency data from file",freqs.file))
load(freqs.file)
print(paste("Loaded frequency data from file",freqs.file))
# Select frequency database. THIS IS REQUIRED
freqs.db <- freqs.trimmed.dense.db
ls(pattern=freqs)
ls(pattern="freqs")
# Define SCORING FUNCTION. Based on Stupid Back Off
#    score(ngram) = counts(ngram)/counts(ngram with first word dropped) if n >1
#    score(1gram) = counts(1gram)/(total number of 1grams (not unique 1grams))
#For stupid back up implementation:
#http://stackoverflow.com/questions/16383194/stupid-backoff-implementation-clarification
# NOTE: Computations here assume that for each sample,
#       the freqs.db is ordered as unigram, bigram, trigram, quadgram...
#
print("Computing ngram scores for stupid backoff scheme.")
scores.db <- lapply(freqs.db,score.sbackoff) # computes scores for each sample
print("Done computing scores.")
str(freqs.trimmed.dense.db)
db.element <- "ten.pct"
freqs.db <- freqs.trimmed.dense.db[[db.element]]
print("Computing ngram scores for stupid backoff scheme.")
scores.db <- lapply(freqs.db,score.sbackoff) # computes scores for each sample
print("Done computing scores.")
str(scores.db)
names(scores.db)
head(scores.db$unigram)
str(freqs.db)
db.element <- "ten.pct"
freqs.db <- list(db.element=freqs.trimmed.dense.db[[db.element]])
str(freqs.db)
db.element <- "ten.pct"
freqs.db <- list()
freqs.db[[db.elemetn]] <- freqs.trimmed.dense.db[[db.element]]
db.element <- "ten.pct"
freqs.db <- list()
freqs.db[[db.element]] <- freqs.trimmed.dense.db[[db.element]]
str(freqs.db)
print("Computing ngram scores for stupid backoff scheme.")
scores.db <- lapply(freqs.db,score.sbackoff) # computes scores for each sample
print("Done computing scores.")
str(scores.db)
head(scores.db$unigram)
head(scores.db$ten.pct$unigram)
head(scores.db$ten.pct$quadgram)
ngrams.db <- list()
bases.db <- list()
ngrams.db[[db.element]] <- ngrams.trimmed.dense.db[[db.element]]
bases.db[[db.element]]  <- bases.trimmed.dense.db[[db.element]]
str(ngrams.db)
#### SELECT FILE FOR SAVING
save.file <- "scoresSB.trimmed.dense.r"
print("Saving scores database, etc... in *scoresSB.r*")
if(file.exists(save.file)){
file.remove(save.file)
}
print(paste("Saving scores database in",save.files))
#### SET NAMES FOR SAVING - REQUIRED
scores.trimmed.dense.db <- scores.db
ngrams.trimmed.dense.db <- ngrams.db
bases.trimmed.dense.db  <- bases.db
str(ngrams.trimmed.dense.db)
print(paste("Saving scores database in",save.file))
getwd()
save(scores.trimmed.dense.db,
ngrams.trimmed.dense.db,
bases.trimmed.dense.db, file=save.file)
print(paste("Finished saving scores database in",save.files))
save(scores.trimmed.dense.db,
ngrams.trimmed.dense.db,
bases.trimmed.dense.db, file=save.file)
print(paste("Finished saving scores database in",save.file))
dir()
print("Completed makeScoresSB.R")
print("Resetting to project directory.")
setwd(prj.dir)
rm(list=ls())
# Capstone Project
# File: makeScoresSB.R
#   Computes the frequencies and scores for stupid backoff
#   Loads document frequency database for computations.
print("Started script: makeScoresSB.R")
# Got to project directory and load tools
prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
setwd(prj.dir)
print(paste("Current directory: ",getwd()))
# load resources
source("nlpTools.R")
print("Loaded tools.")
#Switch to processed corpus directory
source("toProcCorpusDir.R")
print(paste("Switched to directory",getwd()))
#Select freqs directory. THIS IS REQUIRED
freqs.dir <- c("../25.dir")
print(paste("Going to directory containing frequecy data: ",freqs.dir))
setwd(freqs.dir)
print(paste("Switched to directory",getwd()))
###### Select frequency file. THIS IS REQUIRED
freqs.file <- "freqs.trimmed.dense.r"
print(paste("Loading frequency data from file",freqs.file))
load(freqs.file)
print(paste("Loaded frequency data from file",freqs.file))
print(freqs.file)
paste("Loaded frequency data from file",freqs.file)
db.element <- "twentyfive.pct"
freqs.db <- list()
ngrams.db <- list()
bases.db <- list()
freqs.db[[db.element]]  <- freqs.trimmed.dense.db[[db.element]]
ngrams.db[[db.element]] <- ngrams.trimmed.dense.db[[db.element]]
bases.db[[db.element]]  <- bases.trimmed.dense.db[[db.element]]
print("Computing ngram scores for stupid backoff scheme.")
scores.db <- lapply(freqs.db,score.sbackoff) # computes scores for each sample
print("Done computing scores.")
str(scores.db)
#### SELECT FILE FOR SAVING -- REQUIRED
save.file <- "scoresSB.trimmed.dense.r"
#### SET NAMES FOR SAVING - REQUIRED
scores.trimmed.dense.db <- scores.db
ngrams.trimmed.dense.db <- ngrams.db
bases.trimmed.dense.db  <- bases.db
if(file.exists(save.file)){
file.remove(save.file)
}
print(paste("Saving scores database in",save.file))
save(scores.trimmed.dense.db,
ngrams.trimmed.dense.db,
bases.trimmed.dense.db, file=save.file)
print(paste("Finished saving scores database in",save.file))
print("Completed makeScoresSB.R")
print("Resetting to project directory.")
setwd(prj.dir)
rm(list=ls())
source('~/git/NLPCapstone/makeScoresSB.R')
str(scores.trimmed.dense.db)
rm(list=ls())
source('~/git/NLPCapstone/makeScoresSB.R')
