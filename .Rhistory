print("Loaded tools.")
#Switch to processed corpus directory
source("toProcCorpusDir.R")
print(paste("Switched to diretory",getwd()))
dir()
# Loading document text matrices
load("dtms.r")
ls(pattern="dtms")
dtms.var <- ls(pattern="dtms")
print(paste("Loaded document text matrices: ", paste(dtms.var)))
trimDTM <- function(dtm,sparsity=0.33){
dtm.dense <- list()
n <- length(dtm)
dtm.dense[1:n] <- lapply(dtm[1:n],removeSparseTerms,sparsity)
names(dtm.dense) <- names(dtm)
return(dtm.dense)
}
dtms.dense <- lapply(dtms,trimDTM)
str(dtms)
names(dtms)
length(dtms)
names(dtms) <- c("one.pct","five.pct","ten.pct")
file.remove("dtms.r")
save(dtms,file="dtms.r")
names(dtms$one.pct)
dtms.dense <- lapply(dtms,trimDTM)
names(dtms.dense)
str(dtms.dense)
dtms.dense$one.pct
lapply(dtms,function(x) length(names(x$unigram)))
print("Saving dense dtms, etc...")
if(file.exists("dtmsDense.r")){
file.remove("dtmsDense.r")
}
save(dtms.dense,file="dtmsDense.r")
print("Finished saving dtms.dense stuff.")
print("Completed trimDTMS.R")
rm(list=ls())
# Capstone Project
# File: makeScoresSB.R
#   Computes the frequencies and scores for stupid backoff
#   Loads document text matrices
print("Started script: makeScoresSB.R")
# Got to project directory and load tools
prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
setwd(prj.dir)
print(paste("Current directory: ",getwd()))
# load resources
source("nlpTools.R")
print("Loaded tools.")
#Switch to processed corpus directory
source("toProcCorpusDir.R")
print(paste("Switched to diretory",getwd()))
dir()
# Loading document text matrices
load("dtmsDense.r")
ls(pattern="dtms")
dtms.var <- ls(pattern="dtms")
print(paste("Loaded document text matrices: ", paste(dtms.var)))
print(paste("This document text matrix contains:",paste(names(dtms.dense))))
print(paste("This document text matrix contains:",names(dtms.dense)))
# dtms is a list with one element per sample set. Each element has
# DTM for unigram ... quadgram (or higher depending on max.ngram)
print("Computing frequencies and sorting in decreasing order...")
freqs.dense.db <- lapply(dtms.dense,dtm2freq)
names(freqs.dense.db)
head(freqs.dense.db$one.pct$unigram)
head(freqs.dense.db$ten.pct$unigram)
length(names(freqs.dense.db$one.pct$unigram))
length(names(freqs.dense.db$ten.pct$unigram))
ngrams.dense.db <- lapply(freqs.dense.db, # over samples
function(x) lapply(x, # over n-grams of each sample
names))
head(ngrams.dense.db$one.pct$unigram)
head(ngrams.dense.db$ten.pct$unigram)
head(ngrams.dense.db$ten.pct$quadgram)
#     GET number of ngrams for each N, as a list, (includes repetitions)
N.ngrams <- lapply(freqs.dense.db, function(x) lapply(x,sum))
N.ngrams.dense <- lapply(freqs.dense.db, function(x) lapply(x,sum))
rm(N.ngrams)
N.ngrams.dense$one.pct$unigram
N.ngrams.dense$ten.pct$unigram
N.ngrams.dense$ten.pct$quadgram
#     GET number of ngrams for each N, as a list, (no repetitions), i.e. the Vocabulary
V.ngrams.dense <- lapply(ngrams.db,function(x) lapply(x,length))
#     GET number of ngrams for each N, as a list, (no repetitions), i.e. the Vocabulary
V.ngrams.dense <- lapply(ngrams.dense.db,function(x) lapply(x,length))
V.ngrams.dense$one.pct$unigram
v.ngrams.dense$ten.pct$unigram
V.ngrams.dense$ten.pct$unigram
V.ngrams.dense$one.pct$quadgram
V.ngrams.dense$ten.pct$quadgram
rm(N.ngrams.dense,V.ngrams.dense)
N.ngrams.dense.db <- lapply(freqs.dense.db, function(x) lapply(x,sum))
#
#     GET number of ngrams for each N, as a list, (no repetitions), i.e. the Vocabulary
V.ngrams.dense.db <- lapply(ngrams.dense.db,function(x) lapply(x,length))
V.ngrams.dense.db$one.pct$bigram
print("Saving frequencies, etc...")
if(file.exists("freqs.dense.r")){
file.remove("freqs.dense.r")
}
save(freqs.dense.db,ngrams.dense.db,N.ngrams.dense.db,V.ngrams.dense.db,file="freqs.dense.r")
print("Finished saving freqs.dense.db stuff.")
print("Computing ngram scores for stupid backoff scheme.")
scores.dense.db <- lapply(freqs.dense.db,score.sbackoff) # computes scores for each sample
print("Done computing scores.")
names(scores.dense.db)
names(scores.dense.db$one.pct)
head(scores.dense.db$one.pct$unigram)
print("Saving scores database in *scoresSB.dense.r*")
save(scores.dense.db, ngrams.dense.db, file="scoresSB.dense.r")
print("Finished saving scores database!")
print("Completed makeScoresSB.R")
print("Resetting to project directory.")
setwd(prj.dir)
# Capstone Project
# File: benchDB.R
#   builds the benchmark database
#   Loads Corpus, cleans it, generates DTM, DFM (frequency), DSM (score)
#   Loads Corpus (files created by fixRawBench.R)
#     bench/unix.xx.txt
print("Started script: benchDB.R")
# Got to project directory and load tools
prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
setwd(prj.dir)
print(paste("Current directory: ",getwd()))
# load resources
source("nlpTools.R")
#Switch to process corpus directory
source("toBenchDir.R")
rm(list=ls())
load("dbddtms.r")
load("dbdtms.r")
length(dtms)
length(dtms[[1]])
str(dtms)
rm(dtms)
rm(list=ls())
# Capstone Project
# File: benchDB.R
#   builds the benchmark database
#   Loads Corpus, cleans it, generates DTM, DFM (frequency), DSM (score)
#   Loads Corpus (files created by fixRawBench.R)
#     bench/unix.xx.txt
print("Started script: benchDB.R")
# Got to project directory and load tools
prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
setwd(prj.dir)
print(paste("Current directory: ",getwd()))
# load resources
source("nlpTools.R")
#Switch to process corpus directory
source("toBenchDir.R")
# bench.dir set in toBenchDir.R
text.files <- dir(".",pattern="unix.*")
print(paste("Directory: ",getwd()))
print(paste("Directory Files: ",paste0(text.files,collapse=", ")))
# select only a fraction of the processed corpus
# In this case 20% of the original data for each file.
# s.text is a list with each element one sample of the corpus
# the corpus consists of 3 files of text.
s.text <- lapply(c(0.20),sampleText,text.files)
# Building corpus from sample files generated by sampleText function.
corpus <- lapply(s.text,buildCorpus)
clean.corpus <- lapply(corpus,purify.corpus)
#  put in file names
# doing for loop because due to scoping rules it does not stick otherwise
for(j in seq(clean.corpus)){
for(k in seq(clean.corpus[[j]])){
meta(clean.corpus[[j]][[k]],"id") <- text.files[k]
}
}
print("Saving raw corpus and clean corpus.")
if(file.exists("corpus_2.r")){
file.remove("corpus_2.r")
}
save(corpus,clean.corpus,file="corpus_2.r")
print("Finished saving corpus stuff.")
# Second, Generate DTMS for clean corpus
print("Generating DTMS for clean corpus.")
dtms <- lapply(clean.corpus,buildDTM) # using default 4-gram as maximum
for(k in seq_along(clean.corpus)){
names(dtms[[k]])<- c("unigram","bigram","trigram","quadgram")
}
print("Saving DTMS.")
if(file.exists("dtms_2.r")){
file.remove("dtms_2.r")
}
save(dtms,file="dtms_2.r")
print("Finished saving DTMS.")
# Create list of quadgrams
quads <- dtms[[1]]$quadgram$dimnames$Terms
if(file.exists("quads_2.r")){
file.remove("quads_2.r")
}
save(quads,file="quads_2.r")
print("Finished saving quadgrams")
### END
print("Returning to main directory!")
setwd(prj.dir)
print(paste("Current directory: ",getwd()))
print("Completed guessDB.R")
library(stringr)
t <- "  this is book boook   to-night can"t wait -- see you!  "
t <- "  this is book boook   to-night can""t wait -- see you!  "
t <- "  this is book boook   to-night can't wait -- see you!  "
t
t.1 <- str_trim(t)
t.1
t.2 <- tolower(t.1)
toWords <- function(ngram) str_split(ngram,pattern=boundary("word"))
toStr <- function(words) lapply(words,str_c,collapse=" ")
dropFirstWord <- function(ngram) {
words <- toWords(ngram)
n <- length(words[[1]]) # no checks! Assumes all ngrams have same n
toStr(lapply(words,function(s) s[2:n]))
}
getLastWord <- function(ngram){
words <- toWords(ngram)
toStr(lapply(words,function(s) { n<-length(s); s[-(1:(n-1))] }))
}
countWords <- function(ngram) lapply(toWords(ngram),length)
toSpace <- function(x, pattern, ...){
return(gsub(pattern," ", x, ...))
}
toNone <- function(x, pattern, ...){
return(gsub(pattern,"", x, ...))
}
CNT.PAT <-"(?<=[a-zA-Z0-9])'(?=[a-zA-Z0-9])|(?<=[a-zA-Z0-9])\"(?=[a-zA-Z0-9])"
## hyphens (-), en-dash(--), em-dash(---)
HYPHEN.PAT <- "(?<=[a-zA-Z0-9 ])-(?=[a-zA-Z0-9 ])"
ENDASH.PAT <- "(?<=[a-zA-Z0-9 ])--(?=[a-zA-Z0-9 ])" # true endash is outside ASCII code
EMDASH.PAT <- "(?<=[a-zA-Z0-9 ])---(?=[a-zA-Z0-9 ])" # true emdash is outside ASCII code
# bad characters.
BAD.PAT <- "[^[:alnum:][:punct:][:space:]]"
PUNC.PAT <- "[[:punct:]]"
NOTALPHA <- "[^[:alpha:][:space:]]"
MULTISPACE <- "[[:space:]+]"
t.3 <- toNone(t.2,pattern=CNT.PAT,perl=TRUE)
t.3
t.4 <- toNone(t.3,pattern=HYPHEN.PAT,perl=TRUE)
t.4
t.5 <- toSpace(t.4,pattern=ENDASH.PAT,perl=TRUE)
t.5
t.6 <- toSpace(t.5,pattern=EMDASH.PAT,perl=TRUE)
t.6
t.7 <- toNone(t.6,pattern=NOTALPHA)
t.7
t.8 <- toSpace(t.7,pattern=MULTISPACE)
t.8
MULTISPACE <- "[[:space:]]+"
t.8 <- toSpace(t.7,pattern=MULTISPACE)
t.8
simple.purify <- function(ngram){
# takes ngram and return a "cleaned version"
# regex
## contraction pattern
CNT.PAT <-"(?<=[a-zA-Z0-9])'(?=[a-zA-Z0-9])|(?<=[a-zA-Z0-9])\"(?=[a-zA-Z0-9])"
## hyphens (-), en-dash(--), em-dash(---)
HYPHEN.PAT <- "(?<=[a-zA-Z0-9 ])-(?=[a-zA-Z0-9 ])"
ENDASH.PAT <- "(?<=[a-zA-Z0-9 ])--(?=[a-zA-Z0-9 ])" # true endash is outside ASCII code
EMDASH.PAT <- "(?<=[a-zA-Z0-9 ])---(?=[a-zA-Z0-9 ])" # true emdash is outside ASCII code
# bad characters.
BAD.PAT <- "[^[:alnum:][:punct:][:space:]]"
PUNC.PAT <- "[[:punct:]]"
NOTALPHA <- "[^[:alpha:][:space:]]"
MULTISPACE <- "[[:space:]]+"
ngram <- toNone(tolower(str_trim(ngram)),pattern=CNT.PAT,perl=TRUE)
ngram <- toNone(ngram,pattern=HYPHEN.PAT,perl=TRUE)
ngram <- toSpace(ngram,pattern=ENDASH.PAT,perl=TRUE)
ngram <- toSpace(ngram,pattern=EMDASH.PAT,perl=TRUE)
ngram <- toNone(ngram,pattern=NOTALPHA) # gets rid of punctuation and numbers
ngram <- toSpace(ngram,pattern=MULTISPACE) #replaces multi space with on space
return(ngram)
}
t
simple.purify(t)
simple.purify("9 weeks in \r \t\t   ! Doesn't seem to wrok")
simple.purify <- function(ngram){
# takes ngram and return a "cleaned version"
# regex
## contraction pattern
CNT.PAT <-"(?<=[a-zA-Z0-9])'(?=[a-zA-Z0-9])|(?<=[a-zA-Z0-9])\"(?=[a-zA-Z0-9])"
## hyphens (-), en-dash(--), em-dash(---)
HYPHEN.PAT <- "(?<=[a-zA-Z0-9 ])-(?=[a-zA-Z0-9 ])"
ENDASH.PAT <- "(?<=[a-zA-Z0-9 ])--(?=[a-zA-Z0-9 ])" # true endash is outside ASCII code
EMDASH.PAT <- "(?<=[a-zA-Z0-9 ])---(?=[a-zA-Z0-9 ])" # true emdash is outside ASCII code
# bad characters.
BAD.PAT <- "[^[:alnum:][:punct:][:space:]]"
PUNC.PAT <- "[[:punct:]]"
NOTALPHA <- "[^[:alpha:][:space:]]"
MULTISPACE <- "[[:space:]]+"
ngram <- toNone(tolower(ngram),pattern=CNT.PAT,perl=TRUE)
ngram <- toNone(ngram,pattern=HYPHEN.PAT,perl=TRUE)
ngram <- toSpace(ngram,pattern=ENDASH.PAT,perl=TRUE)
ngram <- toSpace(ngram,pattern=EMDASH.PAT,perl=TRUE)
ngram <- toNone(ngram,pattern=NOTALPHA) # gets rid of punctuation and numbers
ngram <- toSpace(ngram,pattern=MULTISPACE) #replaces multi space with on space
return(str_trim(ngram))
}
simple.purify("9 weeks in \r \t\t   ! Doesn't seem to wrok")
simple.purify("\765")
simple.purify <- function(ngram){
# takes ngram and return a "cleaned version"
# regex
## contraction pattern
CNT.PAT <-"(?<=[a-zA-Z0-9])'(?=[a-zA-Z0-9])|(?<=[a-zA-Z0-9])\"(?=[a-zA-Z0-9])"
## hyphens (-), en-dash(--), em-dash(---)
HYPHEN.PAT <- "(?<=[a-zA-Z0-9 ])-(?=[a-zA-Z0-9 ])"
ENDASH.PAT <- "(?<=[a-zA-Z0-9 ])--(?=[a-zA-Z0-9 ])" # true endash is outside ASCII code
EMDASH.PAT <- "(?<=[a-zA-Z0-9 ])---(?=[a-zA-Z0-9 ])" # true emdash is outside ASCII code
# bad characters.
BAD.PAT <- "[^[:alnum:][:punct:][:space:]]"
PUNC.PAT <- "[[:punct:]]"
NOTALPHA <- "[^[:alpha:][:space:]]"
MULTISPACE <- "[[:space:]]+"
ngram <- toNone(ngram,pattern=NOTALPHA) # gets rid of punctuation and numbers
ngram <- toNone(ngram,pattern=CNT.PAT,perl=TRUE)
ngram <- toNone(ngram,pattern=HYPHEN.PAT,perl=TRUE)
ngram <- toSpace(ngram,pattern=ENDASH.PAT,perl=TRUE)
ngram <- toSpace(ngram,pattern=EMDASH.PAT,perl=TRUE)
ngram <- toSpace(ngram,pattern=MULTISPACE) #replaces multi space with on space
return(str_trim(tolower(ngram)))
}
simple.purify("\765")
rm(list=ls())
# Change directory to location of database
prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
download.dir <- "nlpData.dir"; sub.dir <- "final"; proc.corpus.dir <- "proc"
proc.corpus.dir <- file.path(prj.dir,download.dir,sub.dir,proc.corpus.dir)
setwd(proc.corpus.dir)
print(paste("Current directory: ",getwd()))
dir()
load("scoresSB.dense.r")
ls("dense")
ls(pattern="dense")
# automating testing
testing <- function(test.s,a=0.4){
hit <- function(x){
guesses <- guess.sb(unlist(dropLastWord(x)),alpha=a)$guess
if(unlist(getLastWord(x)) %in% guesses){
return(TRUE)
} else {
return(FALSE)
}
}
test.f <- sapply(test.s,hit)
return(test.f)
}
# Change directory to location of project directory and load tools
prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
setwd(prj.dir); getwd()
print("Loading guess function")
source("guess.R")
# Change directory to location of project directory and load tools
prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
setwd(prj.dir); getwd()
print("Loading guess function")
source("guess.R")
print("Loading data for testing!")
if(file.exists(file.path(prj.dir,"sample_quads_dev.r"))){
load(file.path(prj.dir,"sample_quads_dev.r"))
print("Loaded (sample_quads_dev.r) development quadgrams for testing.")
} else { # not present produce it
load(file.path(prj.dir,"quads_dev.r"))
set.seed(1974) # for reproducibility
n.sample <- 1000
test.sample <- quads[sample.int(length(quads),n.sample)]
save(test.sample,file=file.path(prj.dir,"test_quads_dev.r"))
print("Built and saved, test_quads_dev.r, new set of quadgrams from development set for testing")
}
print("Loading data for testing!")
if(file.exists(file.path(prj.dir,"test_quads_dev.r"))){
load(file.path(prj.dir,"test_quads_dev.r"))
print("Loaded (test_quads_dev.r) development quadgrams for testing.")
} else { # not present produce it
load(file.path(prj.dir,"quads_dev.r"))
set.seed(1974) # for reproducibility
n.sample <- 1000
test.sample <- quads[sample.int(length(quads),n.sample)]
save(test.sample,file=file.path(prj.dir,"test_quads_dev.r"))
print("Built and saved, test_quads_dev.r, new set of quadgrams from development set for testing")
}
# Change directory to corpus directory
print("Loading scoresSB.dense.r database ...")
download.dir <- "nlpData.dir"; sub.dir <- "final"; proc.corpus.dir <- "proc"
proc.corpus.dir <- file.path(prj.dir,download.dir,sub.dir,proc.corpus.dir)
setwd(proc.corpus.dir); getwd()
load("scoresSB.dense.r") # loads variable short.dbs
print("Finished loading scoresSB.dense.r databases!")
# loading scores db from 1 pct corpus with sparsity = 0
scoresDB <- scores.dense.db$one.pct ; TOPUNI.SCORES <- scoresDB$unigram[1:3]
ngramsDB <- ngrams.dense.db$one.pct
alphas <- c(0.2,0.4,0.6,0.8)
print("Test 1: input: sample_quads_dev.r; database: 1pct; method: stupid backoff")
for(alpha in alphas){
results <- testing(test.sample,a=alpha)
print(paste("Hits: ",sum(results)," out of ",length(results),"alpha=",alpha))
}
# Change directory to location of project directory and load tools
prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
setwd(prj.dir); getwd()
print("Loading guess function")
source("guess.R")
for(alpha in alphas){
results <- testing(test.sample,a=alpha)
print(paste("Hits: ",sum(results)," out of ",length(results),"alpha=",alpha))
}
testing <- function(test.s,a=0.4){
hit <- function(x,a=a){
guesses <- guess.sb(unlist(dropLastWord(x)),alpha=a)$guess
if(unlist(getLastWord(x)) %in% guesses){
return(TRUE)
} else {
return(FALSE)
}
}
test.f <- sapply(test.s,hit,a)
return(test.f)
}
for(alpha in alphas){
results <- testing(test.sample,a=alpha)
print(paste("Hits: ",sum(results)," out of ",length(results),"alpha=",alpha))
}
testing <- function(test.s,a=0.4){
hit <- function(x,a=a){
guesses <- guess.sb(unlist(dropLastWord(x)),alpha=a)$guess
if(unlist(getLastWord(x)) %in% guesses){
return(TRUE)
} else {
return(FALSE)
}
}
test.f <- sapply(test.s,hit,a=a)
return(test.f)
}
for(alpha in alphas){
results <- testing(test.sample,a=alpha)
print(paste("Hits: ",sum(results)," out of ",length(results),"alpha=",alpha))
}
guess.sb
vec_guess.sb <- function(ngram,dict=ngramsDB$unigram,alpha=0.4){
scores <- vec_score.sb(paste(ngram,dict),alpha=alpha)
out <- top3(scores)
guesses <- unlist(getLastWord(names(out)))
data.frame(guess=guesses[1:3],scores=out[1:3],row.names=c("1st","2nd","3rd"))
}
guess.sb <- vec_guess.sb
for(alpha in alphas){
results <- testing(test.sample,a=alpha)
print(paste("Hits: ",sum(results)," out of ",length(results),"alpha=",alpha))
}
toWords <- function(ngram) str_split(ngram,pattern=boundary("word"))
toStr <- function(words) lapply(words,str_c,collapse=" ")
dropLastWord <- function(ngram){
words <- toWords(ngram)
toStr(lapply(words,function(s) { n<-length(s); s[1:(n-1)] }))
}
getLastWord <- function(ngram){
words <- toWords(ngram)
toStr(lapply(words,function(s) { n<-length(s); s[-(1:(n-1))] }))
}
dropFirstWord <- function(ngram) {
words <- toWords(ngram)
toStr(lapply(words,function(s) { n<-length(s); s[2:n] }))
}
countWords <- function(ngram) lapply(toWords(ngram),length)
toSpace <- function(x, pattern, ...){
return(gsub(pattern," ", x, ...))
}
toNone <- function(x, pattern, ...){
return(gsub(pattern,"", x, ...))
}
for(alpha in alphas){
results <- testing(test.sample,a=alpha)
print(paste("Hits: ",sum(results)," out of ",length(results),"alpha=",alpha))
}
TOP.UNI.SCORES <- scoresDB$unigram[1:3]
ngramsDB <- ngrams.dense.db$one.pct
for(alpha in alphas){
results <- testing(test.sample,a=alpha)
print(paste("Hits: ",sum(results)," out of ",length(results),"alpha=",alpha))
}
length(ngramDB$unigram)
length(ngramsDB$unigram)
names(scores.dense.db)
scoresDB <- scores.dense.db$five.pct ; TOP.UNI.SCORES <- scoresDB$unigram[1:3]
ngramsDB <- ngrams.dense.db$five.pct
alphas <- c(0.2,0.4,0.6,0.8)
length(ngramsDB$unigram)
alphas <- c(0.2,0.4,0.6,0.8)
print("Test 1: input: sample_quads_dev.r; database: 5pct; method: stupid backoff")
for(alpha in alphas){
results <- testing(test.sample,a=alpha)
print(paste("Hits: ",sum(results)," out of ",length(results),"alpha=",alpha))
}
names(scores.dense.db)
scoresDB <- scores.dense.db$ten.pct ; TOP.UNI.SCORES <- scoresDB$unigram[1:3]
ngramsDB <- ngrams.dense.db$ten.pct
alphas <- c(0.2,0.4,0.6,0.8)
length(ngramsDB$unigram)
print("Test 1: input: sample_quads_dev.r; database: 10pct; method: stupid backoff")
for(alpha in alphas){
results <- testing(test.sample,a=alpha)
print(paste("Hits: ",sum(results)," out of ",length(results),"alpha=",alpha))
}
countWords <- function(ngram) lapply(toWords(ngram),length)
countWords("book book")
c("one","two") %in% c("three","four","two")
c("one","two") == c("three","four","two")
"one" == c("three","four")
