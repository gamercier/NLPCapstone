library(stringr)
library(devtools)
install_github("yihui/printr")
library(wordcloud)
library(tm)
library(stringi)
library(tm)
vignette(tm)
vignette("tm")
getwd()
print(paste("Using directory",getwd()))
# Capstone Project
# File: trimFreqV.R
#   Trims the Frequency vector by dropping elements thate occur less than a user
#   defined threshold
print("Started script: trimFreqV.R")
# Got to project directory and load tools
prj.dir <- file.path(Sys.getenv("HOME"),"git","NLPCapstone")
setwd(prj.dir)
print(paste("Current directory: ",getwd()))
# load resources
source("nlpTools.R")
print("Loaded tools.")
#Switch to processed corpus directory
source("toProcCorpusDir.R")
print(paste("Switched to diretory",getwd()))
# Loading document text matrices
load("freqs.dense.r")
freqs <- ls(pattern="freqs")
print(paste("Loaded frequency vectors: ", paste(freqs)))
### ALL THIS PRESUMES THE FREQUENCY DATA IS SORTED, MOST FREQUENT FIRST!!!!!! ####
print("****** WARNING: PRESUMES FREQUENCY DATA IS SORTED, MOST FREQUENT FIRST *****")
freqs <- freqs.dense.db$fifty.pct # freqs for one sample.
ngrams <- ngrams.dense.db$fifty.pct # ngrams for one sample, the ngrams with frequency in freqs
N.ngrams <- N.ngrams.dense.db$fifty.pct
V.ngrams <- V.ngrams.dense.db$fifty.pct
print("**** Statistics on the Frequency Data ****")
print("******************************************")
print("----- Totals -----")
total.ngrams <- as.vector(unlist(N.ngrams))
names(total.ngrams) <- names(N.ngrams)
voc.ngrams <- as.vector(unlist(V.ngrams.dense.db))
names(voc.ngrams) <- names(V.ngrams)
totals.df <- data.frame(n.voc=voc.ngrams,n.ngrams=total.ngrams)
print(totals.df)
# n.voc n.ngrams
# unigram   68915 47756139
# bigram   570854 34343231
# trigram  575428 13762313
# quadgram 224186  3198596
for(ngram in names(freqs)){
print(paste("Minimum frequency in ",ngram," is ",min(freqs[[ngram]])))
}
# [1] "Minimum frequency in  unigram  is  3"
# [1] "Minimum frequency in  bigram  is  3"
# [1] "Minimum frequency in  trigram  is  3"
# [1] "Minimum frequency in  quadgram  is  3"
print("Computing and Plotting Frequency of Frequencies!")
ff <- list(unigram=NULL,bigram=NULL,trigram=NULL,quadgram=NULL)
for(ngram in names(freqs)){
ff[[ngram]] <- freq2ff(freqs[[ngram]])
}
#plotting histograms
for(ngram in names(ff)){
plot(ff[[ngram]][1:200,],main=paste("Frequency of Frequencies - ",ngram))
}
print("Computing and Plotting Fractional Cummulative Frequency")
frac.cum <- lapply(freqs, function(x) cumsum(x)/sum(x))
for(ngram in names(frac.cum)){
plot(x=1:1000,y=frac.cum[[ngram]][1:1000],
main=paste("Fractional Cummulative Frequency - ",ngram),
xlab="index",ylab="cummulative frequency")
}
# scheme one:
#   keep words that make up 90% of the total number of unigrams = dict.trimmed
#   drop all bigrams, etc that contain words outside the dict.trimmed
findIndex <- function(freq.unigram, pct=0.99){
# at and below this index you have 0.9 of the total number of unigrams
fracCum.unigram <- cumsum(freq.unigram)/sum(freq.unigram)
return(sum((fracCum.unigram <= pct)))
}
divideWords <- function(freq.unigram, pct=0.99){
idx <- findIndex(freq.unigram,pct)
words <- names(freq.unigram)
return(list(words.to.keep=words[1:idx],
words.to.cut =words[(idx+1):length(words)],index=idx))
}
pct.to.keep <- 0.99
word.partition <- divideWords(freqs$unigram,pct.to.keep)
print(paste("Index for word (unigram) covering",pct.to.keep*100,"% is",
word.partition$index))
print(paste("Confirmation from fractional cummulative frequency:",
round(frac.cum$unigram[word.partition$index],3)*100,"%"))
cut.index <- word.partition$index
bad.words <- word.partition$words.to.cut
max(freqs$unigram[bad.words])
print(paste("Maximum count of words to cut is", max(freqs$unigram[bad.words])))
print(paste("Maximum count of words to cut is", max(freqs$unigram[bad.words])),".")
print(paste("Maximum count of words to cut is", max(freqs$unigram[bad.words]),"."))
bad.word.max.count <- max(freqs$unigram[bad.words])
print(paste("Maximum count of words to cut is",bad.word.max.count ,"."))
print("Dropping all ngrams with frequency less than this value.")
freqs.trimmed <- vector(mode="list",length=4)
names(freqs.trimmed) <- name(freqs)
freqs.trimmed <- vector(mode="list",length=4)
names(freqs.trimmed) <- names(freqs)
freqs.trimmed
freqs.trimmed$unigram <- freqs$unigram[1:word.partition$index]
min(freqs.trimmed$unigram)
length(freqs.trimmed$unigram)
for(n in seq(2,length(freqs.trimmed))){
freqs.trimmed[[n]] <- freqs[[n]][freqs[[n]] > bad.word.max.count ]
}
sapply(freqs.trimmed,length)
sapply(freqs, length)
ls(pattern="base")
str(bases.dense.db)
ngrams.trimmed <- lapply(freqs.trimmed,names)
str(ngrams.trimmed)
names(ngrams.trimmed[2:length(ngrams.trimmed)])
bases.trimmed <- vector(mode="list",length=3)
for(n in seq(1,length(ngrams.trimmed)-1)){
bases.trimmed[[n]] <- sapply(ngrams.trimmed[[(n+1)]],dropLastWord)
}
names(bases.trimmed) <- names(ngrams.trimmed[2:length(ngrams.trimmed)])
str(bases.trimmed)
str(bases.trimmed)
bases.trimmed <- vector(mode="list",length=3)
for(n in seq(1,length(ngrams.trimmed)-1)){
bases.trimmed[[n]] <- lapply(ngrams.trimmed[[(n+1)]],dropLastWord)
}
names(bases.trimmed) <- names(ngrams.trimmed[2:length(ngrams.trimmed)])
str(bases.trimmed)
bases.trimmed <- vector(mode="list",length=3)
bases.trimmed <- lapply(ngrams.trimmed[2:length(ngrams.trimmed)],dropLastWord)
str(bases.trimmed)
bases.trimmed <- vector(mode="list",length=3)
bases.trimmed <- lapply(ngrams.trimmed[2:length(ngrams.trimmed)],
function(x) unlist(dropLastWord(x)))
str(bases.trimmed)
str(ngrams.trimmed)
ls(pattern="ngrams")
str(N.grams.dense.db)
str(N.ngrams.dense.db)
str(V.ngrams)
N.ngrams.trimmed <- lapply(freqs.trimmed,sum)
V.ngrams.trimmed <- lapply(ngrams.trimmed,length)
str(N.ngrams.trimmed)
str(V.ngrams)
str(V.ngrams.trimmed)
getwd()
dir()
N.ngrams.dense.trimmed <- lapply(freqs.trimmed,sum)
V.ngrams.dense.trimmed <- lapply(ngrams.trimmed,length)
N.ngrams.trimmed <- lapply(freqs.trimmed,sum)
V.ngrams.trimmed <- lapply(ngrams.trimmed,length)
N.ngrams.trimmed.dense.db <- list(fifty.pct=N.ngrams.trimmed)
V.ngrams.trimmed.dense.db <- list(fifty.pct=V.ngrams.trimmed)
save(freqs.trimmed.dense.db,
ngrams.trimmed.dense.db,
bases.trimmed.dense.db,
N.ngrams.trimmed.dense.db,
V.ngrams.trimmed.dense.db, "freqs.trimmed.dense.r")
save(freqs.trimmed.dense.db,
ngrams.trimmed.dense.db,
bases.trimmed.dense.db,
N.ngrams.trimmed.dense.db,
V.ngrams.trimmed.dense.db, file="freqs.trimmed.dense.r")
freqs.trimmed.dense.db <- list(fifty.pct=freqs.trimmed)
ngrams.trimmed.dense.db <- list(fifty.pct=ngrams.trimmed)
bases.trimmed.dense.db <- list(fifty.pct=bases.trimmed)
N.ngrams.trimmed <- lapply(freqs.trimmed,sum)
V.ngrams.trimmed <- lapply(ngrams.trimmed,length)
N.ngrams.trimmed.dense.db <- list(fifty.pct=N.ngrams.trimmed)
V.ngrams.trimmed.dense.db <- list(fifty.pct=V.ngrams.trimmed)
save(freqs.trimmed.dense.db,
ngrams.trimmed.dense.db,
bases.trimmed.dense.db,
N.ngrams.trimmed.dense.db,
V.ngrams.trimmed.dense.db, file="freqs.trimmed.dense.r")
print("Finished trimming with scheme one.")
print("Returning to top directory")
setwd(prj.dir)
print(getwd())
install.packages("StanfordCoreNLP",repos="http://datacube.wu.ac.at",type="source")
install.packages("xml2")
install.packages("XML2R")
install.packages("xml")
library(xml2)
check.quads <- ngrams.trimmed$quadgram %in% ngrams.trimmed$unigram
length(check.quads)
length(ngrams.trimmed$quadgram)
sum(!check.quads)
head(check.quads)
quad.words <- unique(unlist(toWords(ngrams.trimmed$quadram)))
head(quad.words)
t <- unlist(toWords(ngrams.trimmed$quadram))
head(t)
quad.words <- unique(unlist(toWords(ngrams.trimmed$quadgram)))
head(quad.words)
length(quad.words)
tri.words <- unique(unlist(toWords(ngrams.trimmed$trigram)))
bi.words <- unique(unlist(toWords(ngrams.bimmed$trigram)))
tri.words <- unique(unlist(toWords(ngrams.trimmed$trigram)))
bi.words <- unique(unlist(toWords(ngrams.trimmed$bigram)))
sum(!(quad.words %in% ngrams.trimmed$unigram))
sum(!(tri.words %in% ngrams.trimmed$unigram))
sum(!(bi.words %in% ngrams.trimmed$unigram))
print(paste("The number of missing words in ngram=",c(2,3,4),"is",
c(no.bad.bis.words,no.bad.tris.words,no.bad.quad.words)))
no.bad.quads.words <- sum(!(quad.words %in% ngrams.trimmed$unigram))
no.bad.tris.words <- sum(!(tri.words %in% ngrams.trimmed$unigram))
no.bad.bis.words <- sum(!(bi.words %in% ngrams.trimmed$unigram))
print(paste("The number of missing words in ngram=",c(2,3,4),"is",
c(no.bad.bis.words,no.bad.tris.words,no.bad.quad.words)))
print(paste("The number of missing words in ngram=",c(2,3,4),"is",
c(no.bad.bis.words,no.bad.tris.words,no.bad.quads.words)))
